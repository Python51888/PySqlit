# PySQLit é«˜çº§ä¸»é¢˜æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£æ¶µç›– PySQLit çš„é«˜çº§ä½¿ç”¨åœºæ™¯ã€æ€§èƒ½è°ƒä¼˜ã€æ‰©å±•å¼€å‘å’Œç”Ÿäº§éƒ¨ç½²ç­‰ä¸»é¢˜ï¼Œé€‚åˆæœ‰ç»éªŒçš„å¼€å‘è€…å’Œæ¶æ„å¸ˆã€‚

## ğŸ“Š æ€§èƒ½è°ƒä¼˜æ·±åº¦æŒ‡å—

### 1. å†…å­˜ä¼˜åŒ–ç­–ç•¥

#### 1.1 å†…å­˜ä½¿ç”¨åˆ†æ
```python
import tracemalloc
from pysqlit.database import EnhancedDatabase

# å¯åŠ¨å†…å­˜è·Ÿè¸ª
tracemalloc.start()

# åˆ›å»ºæ•°æ®åº“å¹¶æ‰§è¡Œæ“ä½œ
db = EnhancedDatabase("large_dataset.db")

# åˆ†æå†…å­˜ä½¿ç”¨
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')

print("å†…å­˜ä½¿ç”¨TOP 10:")
for stat in top_stats[:10]:
    print(stat)
```

#### 1.2 ç¼“å­˜è°ƒä¼˜
```python
from pysqlit.config import DatabaseConfig

# æ ¹æ®æ•°æ®é‡è°ƒæ•´ç¼“å­˜
config = DatabaseConfig(
    cache_size=1000,  # å¤§æ•°æ®é›†ä½¿ç”¨æ›´å¤§ç¼“å­˜
    page_size=8192,   # å¤§é¡µå‡å°‘I/Oæ¬¡æ•°
    max_memory_usage="512MB"  # é™åˆ¶å†…å­˜ä½¿ç”¨
)

db = EnhancedDatabase("bigdata.db", config=config)
```

#### 1.3 æ‰¹é‡æ“ä½œä¼˜åŒ–
```python
# æœ€ä¼˜æ‰¹é‡å¤§å°æµ‹è¯•
def find_optimal_batch_size(db, data_size=10000):
    batch_sizes = [100, 500, 1000, 2000, 5000]
    results = {}
    
    for batch_size in batch_sizes:
        start = time.time()
        
        for i in range(0, data_size, batch_size):
            batch = data[i:i+batch_size]
            db.executemany(
                "INSERT INTO test VALUES (?, ?, ?)",
                batch
            )
        
        elapsed = time.time() - start
        results[batch_size] = elapsed
    
    return min(results, key=results.get)
```

### 2. å¹¶å‘æ€§èƒ½ä¼˜åŒ–

#### 2.1 è¿æ¥æ± é«˜çº§é…ç½®
```python
from pysqlit.pool import ConnectionPool, PoolConfig

# é«˜çº§è¿æ¥æ± é…ç½®
config = PoolConfig(
    max_connections=50,
    min_connections=5,
    connection_timeout=30,
    idle_timeout=300,
    max_lifetime=3600,
    health_check_interval=60
)

pool = ConnectionPool(
    database_path="production.db",
    config=config
)

# ç›‘æ§è¿æ¥æ± çŠ¶æ€
def monitor_pool(pool):
    stats = pool.get_stats()
    print(f"""
    æ´»è·ƒè¿æ¥: {stats['active_connections']}
    ç©ºé—²è¿æ¥: {stats['idle_connections']}
    ç­‰å¾…è¿æ¥: {stats['waiting_connections']}
    æ€»è¿æ¥æ•°: {stats['total_connections']}
    """)
```

#### 2.2 è¯»å†™åˆ†ç¦»ç­–ç•¥
```python
class ReadWriteRouter:
    """è¯»å†™åˆ†ç¦»è·¯ç”±å™¨"""
    
    def __init__(self, master_db, replica_dbs):
        self.master = master_db
        self.replicas = replica_dbs
        self.current_replica = 0
    
    def get_read_db(self):
        """è·å–è¯»æ•°æ®åº“"""
        replica = self.replicas[self.current_replica]
        self.current_replica = (self.current_replica + 1) % len(self.replicas)
        return replica
    
    def get_write_db(self):
        """è·å–å†™æ•°æ®åº“"""
        return self.master
    
    def execute_read(self, sql, params=()):
        return self.get_read_db().execute(sql, params)
    
    def execute_write(self, sql, params=()):
        return self.master.execute(sql, params)
```

### 3. æŸ¥è¯¢ä¼˜åŒ–é«˜çº§æŠ€å·§

#### 3.1 ç´¢å¼•ç­–ç•¥è®¾è®¡
```python
class IndexOptimizer:
    """ç´¢å¼•ä¼˜åŒ–å™¨"""
    
    def analyze_query_patterns(self, queries):
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        patterns = {}
        for query in queries:
            # æå–WHEREæ¡ä»¶
            where_conditions = self.extract_where_conditions(query)
            # æå–JOINæ¡ä»¶
            join_conditions = self.extract_join_conditions(query)
            # æå–ORDER BY
            order_columns = self.extract_order_columns(query)
            
            patterns[query] = {
                'where': where_conditions,
                'join': join_conditions,
                'order': order_columns
            }
        return patterns
    
    def recommend_indexes(self, patterns):
        """æ¨èç´¢å¼•"""
        recommendations = []
        
        for query, pattern in patterns.items():
            # åŸºäºæŸ¥è¯¢é¢‘ç‡å’Œé€‰æ‹©æ€§æ¨èç´¢å¼•
            if pattern['where']:
                for col, op in pattern['where']:
                    if self.is_selective(col):
                        recommendations.append({
                            'table': self.get_table_from_query(query),
                            'columns': [col],
                            'type': 'btree',
                            'reason': f'WHERE {col} {op}'
                        })
        
        return recommendations
```

#### 3.2 æŸ¥è¯¢é‡å†™ä¼˜åŒ–
```python
class QueryRewriter:
    """æŸ¥è¯¢é‡å†™å™¨"""
    
    def rewrite_subquery_to_join(self, query):
        """å°†å­æŸ¥è¯¢é‡å†™ä¸ºJOIN"""
        # ç¤ºä¾‹ï¼šå°†INå­æŸ¥è¯¢é‡å†™ä¸ºJOIN
        # åŸæŸ¥è¯¢: SELECT * FROM users WHERE id IN (SELECT user_id FROM orders)
        # é‡å†™å: SELECT DISTINCT u.* FROM users u JOIN orders o ON u.id = o.user_id
        pass
    
    def optimize_pagination(self, query, offset, limit):
        """ä¼˜åŒ–åˆ†é¡µæŸ¥è¯¢"""
        # ä½¿ç”¨é”®é›†åˆ†é¡µæ›¿ä»£OFFSET
        if offset > 1000:
            return self.keyset_pagination(query, offset, limit)
        return query
```

## ğŸ—ï¸ æ‰©å±•å¼€å‘æŒ‡å—

### 1. è‡ªå®šä¹‰å­˜å‚¨å¼•æ“

#### 1.1 å­˜å‚¨å¼•æ“æ¥å£
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

class StorageEngine(ABC):
    """å­˜å‚¨å¼•æ“æ¥å£"""
    
    @abstractmethod
    def open(self, path: str, **kwargs) -> None:
        """æ‰“å¼€å­˜å‚¨"""
        pass
    
    @abstractmethod
    def close(self) -> None:
        """å…³é—­å­˜å‚¨"""
        pass
    
    @abstractmethod
    def read_page(self, page_num: int) -> bytes:
        """è¯»å–é¡µ"""
        pass
    
    @abstractmethod
    def write_page(self, page_num: int, data: bytes) -> None:
        """å†™å…¥é¡µ"""
        pass
    
    @abstractmethod
    def get_page_count(self) -> int:
        """è·å–é¡µæ•°"""
        pass

class RedisStorageEngine(StorageEngine):
    """Rediså­˜å‚¨å¼•æ“å®ç°"""
    
    def __init__(self, redis_url: str):
        import redis
        self.redis = redis.from_url(redis_url)
    
    def read_page(self, page_num: int) -> bytes:
        key = f"db:page:{page_num}"
        data = self.redis.get(key)
        return data or b''
    
    def write_page(self, page_num: int, data: bytes) -> None:
        key = f"db:page:{page_num}"
        self.redis.set(key, data)
```

#### 1.2 æ³¨å†Œè‡ªå®šä¹‰å¼•æ“
```python
from pysqlit.storage import register_storage_engine

# æ³¨å†ŒRediså­˜å‚¨å¼•æ“
register_storage_engine('redis', RedisStorageEngine)

# ä½¿ç”¨è‡ªå®šä¹‰å¼•æ“
db = EnhancedDatabase("redis://localhost:6379/0", engine='redis')
```

### 2. è‡ªå®šä¹‰ç´¢å¼•ç±»å‹

#### 2.1 ç´¢å¼•æ¥å£è®¾è®¡
```python
from pysqlit.index import IndexInterface

class HashIndex(IndexInterface):
    """å“ˆå¸Œç´¢å¼•å®ç°"""
    
    def __init__(self, storage):
        self.storage = storage
        self.hash_table = {}
    
    def insert(self, key: Any, value: int) -> None:
        """æ’å…¥é”®å€¼å¯¹"""
        if key not in self.hash_table:
            self.hash_table[key] = []
        self.hash_table[key].append(value)
    
    def search(self, key: Any) -> Optional[List[int]]:
        """ç²¾ç¡®æŸ¥æ‰¾"""
        return self.hash_table.get(key)
    
    def range_search(self, start: Any, end: Any) -> List[int]:
        """èŒƒå›´æŸ¥æ‰¾ï¼ˆå“ˆå¸Œç´¢å¼•ä¸æ”¯æŒï¼‰"""
        raise NotImplementedError("Hash index does not support range queries")

class FullTextIndex(IndexInterface):
    """å…¨æ–‡ç´¢å¼•å®ç°"""
    
    def __init__(self, storage):
        self.storage = storage
        self.inverted_index = {}
    
    def insert(self, document: str, doc_id: int) -> None:
        """æ’å…¥æ–‡æ¡£"""
        words = self.tokenize(document)
        for word in words:
            if word not in self.inverted_index:
                self.inverted_index[word] = set()
            self.inverted_index[word].add(doc_id)
    
    def search(self, query: str) -> List[int]:
        """å…¨æ–‡æœç´¢"""
        terms = self.tokenize(query)
        results = set()
        for term in terms:
            if term in self.inverted_index:
                if not results:
                    results = self.inverted_index[term]
                else:
                    results &= self.inverted_index[term]
        return list(results)
```

### 3. æ’ä»¶ç³»ç»Ÿå¼€å‘

#### 3.1 æ’ä»¶æ¶æ„è®¾è®¡
```python
from typing import Protocol
from dataclasses import dataclass

@dataclass
class PluginContext:
    """æ’ä»¶ä¸Šä¸‹æ–‡"""
    database: EnhancedDatabase
    config: Dict[str, Any]
    logger: Any

class Plugin(Protocol):
    """æ’ä»¶åè®®"""
    
    def initialize(self, context: PluginContext) -> None:
        """åˆå§‹åŒ–æ’ä»¶"""
        ...
    
    def before_query(self, sql: str, params: tuple) -> None:
        """æŸ¥è¯¢å‰é’©å­"""
        ...
    
    def after_query(self, sql: str, params: tuple, result: Any, duration: float) -> None:
        """æŸ¥è¯¢åé’©å­"""
        ...
    
    def shutdown(self) -> None:
        """å…³é—­æ’ä»¶"""
        ...

class MetricsPlugin:
    """æŒ‡æ ‡æ”¶é›†æ’ä»¶"""
    
    def __init__(self):
        self.query_count = 0
        self.total_duration = 0.0
    
    def initialize(self, context: PluginContext):
        self.logger = context.logger
    
    def before_query(self, sql: str, params: tuple):
        self.start_time = time.time()
    
    def after_query(self, sql: str, params: tuple, result: Any, duration: float):
        self.query_count += 1
        self.total_duration += duration
        
        if duration > 1.0:  # æ…¢æŸ¥è¯¢
            self.logger.warning(f"Slow query: {duration:.3f}s - {sql}")
```

#### 3.2 æ’ä»¶æ³¨å†Œå’Œä½¿ç”¨
```python
from pysqlit.plugin import PluginManager

# åˆ›å»ºæ’ä»¶ç®¡ç†å™¨
plugin_manager = PluginManager()

# æ³¨å†Œæ’ä»¶
plugin_manager.register_plugin('metrics', MetricsPlugin())
plugin_manager.register_plugin('cache', CachePlugin())

# ä½¿ç”¨æ’ä»¶
db = EnhancedDatabase("app.db", plugins=['metrics', 'cache'])
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²æŒ‡å—

### 1. å®¹å™¨åŒ–éƒ¨ç½²

#### 1.1 Dockeré…ç½®
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p /app/data /app/backups

# è®¾ç½®æƒé™
RUN chmod 755 /app/data /app/backups

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "from pysqlit.database import EnhancedDatabase; db = EnhancedDatabase('/app/data/health.db'); db.execute('SELECT 1'); db.close()"

# è¿è¡Œåº”ç”¨
CMD ["python", "-m", "pysqlit.server", "--host", "0.0.0.0", "--port", "8080"]
```

#### 1.2 Docker Composeé…ç½®
```yaml
# docker-compose.yml
version: '3.8'

services:
  py-sqlit:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./data:/app/data
      - ./backups:/app/backups
    environment:
      - DATABASE_PATH=/app/data/app.db
      - BACKUP_PATH=/app/backups
      - LOG_LEVEL=INFO
    restart: unless-stopped
    
  backup:
    image: py-sqlit:latest
    volumes:
      - ./data:/app/data
      - ./backups:/app/backups
    command: ["python", "-m", "pysqlit.backup", "--schedule", "daily"]
    depends_on:
      - py-sqlit
```

### 2. Kuberneteséƒ¨ç½²

#### 2.1 StatefulSeté…ç½®
```yaml
# k8s/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: py-sqlit
spec:
  serviceName: py-sqlit
  replicas: 1
  selector:
    matchLabels:
      app: py-sqlit
  template:
    metadata:
      labels:
        app: py-sqlit
    spec:
      containers:
      - name: py-sqlit
        image: py-sqlit:latest
        ports:
        - containerPort: 8080
        env:
        - name: DATABASE_PATH
          value: "/data/app.db"
        volumeMounts:
        - name: data
          mountPath: /data
        - name: backups
          mountPath: /backups
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
  - metadata:
      name: backups
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi
```

#### 2.2 é…ç½®æ˜ å°„
```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: py-sqlit-config
data:
  config.yaml: |
    database:
      page_size: 4096
      cache_size: 1000
      max_connections: 50
    
    performance:
      query_timeout: 30
      slow_query_threshold: 1.0
    
    backup:
      enabled: true
      schedule: "0 2 * * *"
      retention_days: 30
```

### 3. ç›‘æ§å’Œå‘Šè­¦

#### 3.1 PrometheusæŒ‡æ ‡
```python
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
query_counter = Counter('pysqlit_queries_total', 'Total queries', ['operation'])
query_duration = Histogram('pysqlit_query_duration_seconds', 'Query duration')
active_connections = Gauge('pysqlit_active_connections', 'Active connections')
database_size = Gauge('pysqlit_database_size_bytes', 'Database size')

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, db):
        self.db = db
    
    def collect_metrics(self):
        """æ”¶é›†æŒ‡æ ‡"""
        # æŸ¥è¯¢è®¡æ•°
        query_counter.labels(operation='select').inc()
        
        # æ•°æ®åº“å¤§å°
        size = self.db.get_database_size()
        database_size.set(size)
        
        # æ´»è·ƒè¿æ¥
        connections = self.db.get_active_connections()
        active_connections.set(connections)
```

#### 3.2 Grafanaä»ªè¡¨æ¿
```json
{
  "dashboard": {
    "title": "PySQLit Monitoring",
    "panels": [
      {
        "title": "Query Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(pysqlit_queries_total[5m])",
            "legendFormat": "{{operation}}"
          }
        ]
      },
      {
        "title": "Database Size",
        "type": "stat",
        "targets": [
          {
            "expr": "pysqlit_database_size_bytes",
            "legendFormat": "Size"
          }
        ]
      }
    ]
  }
}
```

## ğŸ” æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜è¯Šæ–­

#### 1.1 æ€§èƒ½é—®é¢˜
```python
# æ€§èƒ½è¯Šæ–­å·¥å…·
class PerformanceDiagnostics:
    def analyze_slow_queries(self, threshold=1.0):
        """åˆ†ææ…¢æŸ¥è¯¢"""
        slow_queries = []
        
        # è·å–æŸ¥è¯¢æ—¥å¿—
        logs = self.db.get_query_logs()
        
        for log in logs:
            if log.duration > threshold:
                slow_queries.append({
                    'sql': log.sql,
                    'duration': log.duration,
                    'params': log.params,
                    'plan': self.explain_query(log.sql, log.params)
                })
        
        return slow_queries
    
    def check_index_usage(self, table):
        """æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        stats = self.db.execute("""
            SELECT name, stat FROM sqlite_stat1 
            WHERE tbl = ?
        """, (table,))
        
        recommendations = []
        for name, stat in stats:
            if self.is_index_underused(name, stat):
                recommendations.append({
                    'index': name,
                    'reason': 'Underused index',
                    'action': 'Consider dropping'
                })
        
        return recommendations
```

#### 1.2 å†…å­˜é—®é¢˜
```python
# å†…å­˜æ³„æ¼æ£€æµ‹
import gc
import psutil

def check_memory_leaks():
    """æ£€æŸ¥å†…å­˜æ³„æ¼"""
    process = psutil.Process()
    
    # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
    initial_memory = process.memory_info().rss
    
    # æ‰§è¡Œå¤§é‡æ“ä½œ
    db = EnhancedDatabase("test.db")
    for i in range(10000):
        db.execute("INSERT INTO test VALUES (?, ?)", (i, f"data{i}"))
    
    # å…³é—­æ•°æ®åº“
    db.close()
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    
    # æ£€æŸ¥å†…å­˜æ˜¯å¦é‡Šæ”¾
    final_memory = process.memory_info().rss
    
    if final_memory > initial_memory * 1.1:
        print("Potential memory leak detected")
        return False
    
    return True
```

### 2. è°ƒè¯•å·¥å…·

#### 2.1 è°ƒè¯•æ¨¡å¼
```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
import logging
from pysqlit.debug import DebugDatabase

# é…ç½®è°ƒè¯•æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# åˆ›å»ºè°ƒè¯•æ•°æ®åº“
db = DebugDatabase("app.db", debug=True)

# å¯ç”¨æŸ¥è¯¢æ—¥å¿—
db.enable_query_logging()

# è®¾ç½®æ–­ç‚¹
db.set_breakpoint("SELECT * FROM users WHERE id = 42")

# æ‰§è¡ŒæŸ¥è¯¢
result = db.execute("SELECT * FROM users")
```

#### 2.2 æ€§èƒ½åˆ†æ
```python
# ä½¿ç”¨cProfileè¿›è¡Œæ€§èƒ½åˆ†æ
import cProfile
import pstats

def profile_database_operations():
    """åˆ†ææ•°æ®åº“æ“ä½œæ€§èƒ½"""
    profiler = cProfile.Profile()
    
    profiler.enable()
    
    # æ‰§è¡Œæ•°æ®åº“æ“ä½œ
    db = EnhancedDatabase("profile.db")
    db.execute("CREATE TABLE test (id INTEGER, data TEXT)")
    
    for i in range(1000):
        db.execute("INSERT INTO test VALUES (?, ?)", (i, f"data{i}"))
    
    results = db.execute("SELECT * FROM test WHERE id > 500")
    
    profiler.disable()
    
    # è¾“å‡ºåˆ†æç»“æœ
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)
```

## ğŸ“š é«˜çº§ç¤ºä¾‹

### 1. å®æ—¶æ•°æ®ç®¡é“
```python
import asyncio
import aiohttp
from pysqlit.async_support import AsyncDatabase

class RealtimeDataPipeline:
    """å®æ—¶æ•°æ®ç®¡é“"""
    
    def __init__(self, db_path: str):
        self.db = AsyncDatabase(db_path)
        self.buffer = asyncio.Queue(maxsize=1000)
    
    async def ingest_data(self, source_url: str):
        """æ•°æ®æ‘„å–"""
        async with aiohttp.ClientSession() as session:
            async with session.get(source_url) as response:
                async for line in response.content:
                    await self.buffer.put(line)
    
    async def process_data(self):
        """æ•°æ®å¤„ç†"""
        while True:
            batch = []
            for _ in range(100):  # æ‰¹å¤„ç†100æ¡
                try:
                    item = await asyncio.wait_for(
                        self.buffer.get(), 
                        timeout=1.0
                    )
                    batch.append(item)
                except asyncio.TimeoutError:
                    break
            
            if batch:
                await self.db.executemany(
                    "INSERT INTO events (data, timestamp) VALUES (?, ?)",
                    [(item, datetime.now()) for item in batch]
                )
```

### 2. åˆ†å¸ƒå¼ç¼“å­˜å±‚
```python
import redis
from functools import lru_cache

class CachedDatabase:
    """å¸¦ç¼“å­˜çš„æ•°æ®åº“"""
    
    def __init__(self, db_path: str, redis_url: str):
        self.db = EnhancedDatabase(db_path)
        self.cache = redis.from_url(redis_url)
        self.cache_ttl = 300  # 5åˆ†é’Ÿ
    
    @lru_cache(maxsize=1000)
    def get_user(self, user_id: int):
        """è·å–ç”¨æˆ·ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = f"user:{user_id}"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached = self.cache.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # ä»æ•°æ®åº“è·å–
        user = self.db.execute(
            "SELECT * FROM users WHERE id = ?",
            (user_id,)
        )[0]
        
        # å†™å…¥ç¼“å­˜
        self.cache.setex(
            cache_key, 
            self.cache_ttl, 
            json.dumps(user)
        )
        
        return user
```

---

**ä¸‹ä¸€æ­¥**: æŸ¥çœ‹[æ•…éšœæ’é™¤æŒ‡å—](troubleshooting.md)æˆ–[è´¡çŒ®æŒ‡å—](development.md)è·å–æ›´å¤šä¿¡æ¯ï¼